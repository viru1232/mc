import pandas as pd
df = pd.read_csv('SMSSpamCollection', sep='\t', names = ['label', 'text'])     df
df.shape        !pip install nltk    import nltk  nltk.download('punkt')
nltk.download('punkt_tab')      nltk.download('stopwords')

sent = 'Hello Friends! how are you?'
from nltk.tokenize import word_tokenize
     word_tokenize(sent)

from nltk.corpus import stopwords
   swords = stopwords.words('english')      swords 

   clean = [word for word in word_tokenize(sent) if word not in swords]          clean

from nltk.stem import PorterStemmer
ps = PorterStemmer()
clean = [ps.stem(word) for word in word_tokenize(sent)
         if word not in swords]            clean


sent = 'Hello Friends! how are you? we will be learning python todays.'
def clean_text(sent):
    tokens = word_tokenize(sent)
    clean = [word for word in tokens
             if word.isdigit() or word.isalpha()]
    clean = [ps.stem(word) for word in word_tokenize(sent)
         if word not in swords]
    print(clean)


    
clean_text(sent)
#preprocessing

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(analyzer=clean_text)

x = df['text']
y =df['label']

# If x is a pandas Series
x = x.fillna('')  # replace None/NaN with empty string
x = x.astype(str)  # convert all elements to strings



import re

def clean_text(text):
    # Convert None or non-string to empty string
    if not isinstance(text, str):
        text = ""

    # Lowercase
    text = text.lower()

    # Remove non-alphabetic characters
    text = re.sub(r'[^a-z\s]', '', text)

    # Tokenize by splitting on whitespace
    tokens = text.split()

    return tokens  # ALWAYS return a list


from sklearn.feature_extraction.text import TfidfVectorizer

# Ensure all elements are strings
x = [str(i) if i is not None else "" for i in x]

tfidf = TfidfVectorizer(analyzer=clean_text)
x_new = tfidf.fit_transform(x)

print(x_new.shape)                x_new    y.value_counts()


#cross
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_new, y, random_state=0, test_size=0.25)
x_train.shape   x_test.shape

from sklearn.naive_bayes import GaussianNB        nb = GaussianNB()
nb.fit(x_train.toarray(), y_train)

y_pred = nb.predict(x_test.toarray())           y_test.value_counts()



from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay .from_predictions(y_test, y_pred)

from sklearn.metrics import accuracy_score, classification_report
print(classification_report(y_test, y_pred))
accuracy_score(y_test, y_pred)
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=0)

rf.fit(x_train, y_train)
y_pred = rf.predict(x_test)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
print(classification_report(y_test, y_pred))
accuracy_score(y_test, y_pred)

from sklearn.linear_model import LogisticRegression
log = LogisticRegression()
log.fit(x_train, y_train)
y_pred = log.predict(x_test)
accuracy_score(y_test, y_pred)

#hyper tuning

from sklearn.model_selection import GridSearchCV
params= {
    'criterion': ['gini', 'entropy'],
    'max_features': ['sqrt', 'log2'],
    'random_state': [0,1,2,3,4],
    'class_weight':['balanced', 'balanced_subsample']
}

grid = GridSearchCV(rf, param_grid=params, cv=5, scoring='accuracy')
grid.fit(x_train, y_train)
rf = grid.best_estimator_
y_pred = rf.predict(x_test)
accuracy_score(y_test, y_pred)





