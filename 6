# üõçÔ∏è Mall Customer Segmentation using Clustering
# Dataset: https://www.kaggle.com/shwetabh123/mall-customers

# -------------------------------------------------------
# Step 1: Import Required Libraries
# -------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import silhouette_score
import scipy.cluster.hierarchy as sch
%matplotlib inline

# -------------------------------------------------------
# Step 2: Load Dataset
# -------------------------------------------------------
data = pd.read_csv("Mall_Customers.csv")
print("‚úÖ Dataset Loaded Successfully!")
print(data.head())

# -------------------------------------------------------
# Step 3: Data Preprocessing
# -------------------------------------------------------
print("\nDataset Information:")
print(data.info())

# Encode Gender (if needed)
le = LabelEncoder()
data['Gender'] = le.fit_transform(data['Gender'])  # Male=1, Female=0

# Check for null values
print("\nNull Values:\n", data.isnull().sum())

# -------------------------------------------------------
# Step 4: Feature Selection
# -------------------------------------------------------
# We'll use Annual Income and Spending Score for clustering
X = data[['Annual Income (k$)', 'Spending Score (1-100)']]

# Optional: scale data (good practice for distance-based algorithms)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------------------------------------
# Step 5: Train-Test Split (though clustering is unsupervised)
# -------------------------------------------------------
# Splitting just for demonstration as per assignment instruction
X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)
print("\nTraining Samples:", X_train.shape[0])
print("Testing Samples:", X_test.shape[0])

# -------------------------------------------------------
# Step 6: Apply Clustering Algorithms
# -------------------------------------------------------
# ---- KMeans Clustering ----
inertia = []
silhouette_scores = []
for i in range(2, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(X_train)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_train, kmeans.labels_))

# Elbow Method Visualization
plt.figure(figsize=(8,4))
plt.plot(range(2,11), inertia, marker='o')
plt.title("Elbow Method for Optimal k")
plt.xlabel("Number of Clusters")
plt.ylabel("Inertia")
plt.show()

# Silhouette Score Visualization
plt.figure(figsize=(8,4))
plt.plot(range(2,11), silhouette_scores, marker='o', color='green')
plt.title("Silhouette Scores for Different k")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.show()

# Optimal k (based on elbow/silhouette)
k_optimal = 5
kmeans = KMeans(n_clusters=k_optimal, random_state=42)
kmeans.fit(X_scaled)
y_kmeans = kmeans.labels_

# ---- Hierarchical Clustering ----
hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X_scaled)

# -------------------------------------------------------
# Step 7: Evaluate Clustering Models
# -------------------------------------------------------
# Evaluate using silhouette score (higher = better)
silhouette_kmeans = silhouette_score(X_scaled, y_kmeans)
silhouette_hc = silhouette_score(X_scaled, y_hc)

print("\nüìä Model Evaluation:")
print(f"KMeans Silhouette Score: {silhouette_kmeans:.4f}")
print(f"Hierarchical Clustering Silhouette Score: {silhouette_hc:.4f}")

# -------------------------------------------------------
# Step 8: Visualize Clusters (KMeans)
# -------------------------------------------------------
plt.figure(figsize=(8,5))
plt.scatter(X_scaled[y_kmeans == 0, 0], X_scaled[y_kmeans == 0, 1], s=50, label='Cluster 1')
plt.scatter(X_scaled[y_kmeans == 1, 0], X_scaled[y_kmeans == 1, 1], s=50, label='Cluster 2')
plt.scatter(X_scaled[y_kmeans == 2, 0], X_scaled[y_kmeans == 2, 1], s=50, label='Cluster 3')
plt.scatter(X_scaled[y_kmeans == 3, 0], X_scaled[y_kmeans == 3, 1], s=50, label='Cluster 4')
plt.scatter(X_scaled[y_kmeans == 4, 0], X_scaled[y_kmeans == 4, 1], s=50, label='Cluster 5')
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=200, c='black', marker='X', label='Centroids')
plt.title("Customer Segments (KMeans)")
plt.xlabel("Annual Income (Scaled)")
plt.ylabel("Spending Score (Scaled)")
plt.legend()
plt.show()

# -------------------------------------------------------
# Step 9: Visualize Hierarchical Clustering Dendrogram
# -------------------------------------------------------
plt.figure(figsize=(10,6))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title("Dendrogram for Hierarchical Clustering")
plt.xlabel("Customers")
plt.ylabel("Euclidean Distance")
plt.show()

# -------------------------------------------------------
# Step 10: Cross-Validation (using Silhouette Scores)
# -------------------------------------------------------
# Cross-validation is less common in clustering but can be simulated
kf = KFold(n_splits=5, shuffle=True, random_state=42)
sil_scores = []

for train_idx, test_idx in kf.split(X_scaled):
    km = KMeans(n_clusters=k_optimal, random_state=42)
    km.fit(X_scaled[train_idx])
    sil = silhouette_score(X_scaled[test_idx], km.predict(X_scaled[test_idx]))
    sil_scores.append(sil)

print("\nüìà Cross-Validation Silhouette Scores (KMeans):", sil_scores)
print("Mean CV Silhouette Score:", np.mean(sil_scores))

# -------------------------------------------------------
# Step 11: Cluster Insights
# -------------------------------------------------------
# Add cluster labels to the original data
data['Cluster'] = y_kmeans
cluster_summary = data.groupby('Cluster')[['Annual Income (k$)', 'Spending Score (1-100)']].mean()

print("\nüßæ Cluster Summary:")
print(cluster_summary)
